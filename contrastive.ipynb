{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "343eb22b-8ec6-415a-8685-5f6f9fb36bd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from transformers import AutoModel, AutoTokenizer, AdamW\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "\n",
    "from tqdm import tqdm\n",
    "from utils import *\n",
    "from contrastive_utils import *\n",
    "import random\n",
    "# from class_balanced_loss import *\n",
    "\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1241836f-79cb-4032-b7d5-1599acd30dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b82a12e-e0b0-4869-935b-b04a261d6489",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) |available.\n",
      "We will use the GPU: NVIDIA GeForce GTX 1080 Ti\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():    \n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda\")\n",
    "    print('There are %d GPU(s) |available.' % torch.cuda.device_count())\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "# If not...\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a63a3212-b76f-4ea2-8679-2e6e61a9d3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_contrastive(model, dataloader, tokenizer, optimizer, device, epochs,augment,generation_loss_fn):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "        for batch in progress_bar:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            long_answer_ids = batch['long_answer_ids'].to(device)\n",
    "            \n",
    "            # Create two views of the same batch\n",
    "            aug_input_ids_1, aug_attention_mask_1 = augment(input_ids, attention_mask, tokenizer)\n",
    "            aug_input_ids_2, aug_attention_mask_2 = augment(input_ids, attention_mask, tokenizer)\n",
    "            \n",
    "            proj_1,long_answer_logits_1 = model(aug_input_ids_1, aug_attention_mask_1)\n",
    "            proj_2,long_answer_logits_2 = model(aug_input_ids_2, aug_attention_mask_2)\n",
    "            \n",
    "            loss = contrastive_loss(proj_1, proj_2)\n",
    "            generation_loss = generation_loss_fn(long_answer_logits_1.view(-1, long_answer_logits_1.size(-1)), \n",
    "                                                 long_answer_ids.view(-1)) + generation_loss_fn(long_answer_logits_2.view(-1, long_answer_logits_2.size(-1)), \n",
    "                                                 long_answer_ids.view(-1))\n",
    "\n",
    "            loss += generation_loss\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            progress_bar.set_postfix({'loss': loss.item()})\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(dataloader):.4f}\")\n",
    "\n",
    "# train_contrastive(model, train_loader, tokenizer, optimizer, device, epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ad6b569-1793-4984-a730-3c64a758561c",
   "metadata": {},
   "outputs": [],
   "source": [
    "expert_train_processed,artificial_train_processed,unlabeled_processed,expert_test_processed = load_pubmedqa_data(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "679ff2de-f7bb-42cb-b59f-ee6075a909a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_name = \"dmis-lab/biobert-base-cased-v1.1\"\n",
    "# model_name = \"nlpie/bio-mobilebert\"\n",
    "# model_name = 'nlpie/bio-tinybert'\n",
    "model_name = \"nlpie/tiny-biobert\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41f61517-6dbf-492d-8568-fc315758bd9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at nlpie/tiny-biobert and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = PubMedQAContrastive(model_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "83829e25-6b09-4fd5-a7c3-26186167bd88",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = PubMedQADataset(expert_train_processed + artificial_train_processed, tokenizer,max_length = 400)\n",
    "unlabeled_dataset = PubMedQADataset(unlabeled_processed, tokenizer,max_length = 400)\n",
    "test_dataset = PubMedQADataset(expert_test_processed,tokenizer,max_length = 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a045cfc-d1da-4b6e-84ac-08c8c13979e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23087604"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "49e3fc0f-d8ef-4bab-86f3-e84f9b0408ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_class_counts(dataset):\n",
    "    class_counts = Counter()\n",
    "    for data in tqdm(dataset):\n",
    "        label = data['label'].item()\n",
    "        class_counts[label] += 1\n",
    "    return class_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac765d84-2067-4b50-840a-7b9a20dcc09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Compute class counts and weights\n",
    "class_counts = compute_class_counts(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b605cc43-1341-47d8-b711-75fc2641934e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_count_list = [196420, 15294, 55]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cd9270dd-caf3-4f83-a1f5-61d371eaee73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class_count_list = [class_counts[i] for i in range(len(class_counts))]\n",
    "class_weights = [max(class_count_list) / count for count in class_count_list]\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f86015b4-7cda-45a2-9f7a-af7495a23210",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1.0000e+00, 1.2843e+01, 3.5713e+03], device='cuda:0')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a9ef5a16-79a0-4da0-85cd-f8139d77452f",
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "unlabeled_loader = DataLoader(unlabeled_dataset, batch_size=8, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset,batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "49d02838-ab50-42cb-896f-98a2c53c0c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "630fa6c7-eb29-4279-bfeb-586c6548fa60",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "79e74be0-3cf0-40d2-b30d-18306cc55a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "generation_loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a289a977-d970-40e8-ad60-36e2556f3a1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/2: 100%|██████████| 7657/7657 [23:53<00:00,  5.34it/s, loss=3.81e-6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2, Loss: 0.7038\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/2: 100%|██████████| 7657/7657 [23:54<00:00,  5.34it/s, loss=0]    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/2, Loss: 0.6967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_contrastive(model, unlabeled_loader, tokenizer,optimizer, device, \n",
    "                  num_epochs, augment,generation_loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0e1ceb36-3749-450e-8d1b-65cfdcda132c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model.state_dict(), 'pubmedqa_contrastive_model.pth')\n",
    "\n",
    "state = {\n",
    "    'state_dict': model.state_dict(),\n",
    "    'optimizer': optimizer.state_dict(),\n",
    "    'epochs': 4,\n",
    "    'lr':5e-5\n",
    "}\n",
    "\n",
    "torch.save(state, f\"weights/{model_name.split('/')[1]}_contrastive_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b51c5a28-806a-46cc-8574-bdea0f431a6b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d26c68fd-b699-4c63-bd98-533ae5e833bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at nlpie/tiny-biobert and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the trained contrastive model\n",
    "model = PubMedQAContrastive(model_name).to(device)\n",
    "\n",
    "checkpoint = torch.load(f\"weights/{model_name.split('/')[1]}_contrastive_model.pt\")\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "996d21b3-155e-426b-9771-4a3db5f01328",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "# Create and train the classifier\n",
    "classifier = PubMedQAClassifier(model).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a435bc96-1308-424f-8ebe-fb530c04023b",
   "metadata": {},
   "source": [
    "Before pre-training = 0.398\n",
    "\n",
    "\n",
    "After pre-training for 2 epochs = 0.522\n",
    "\n",
    "After pre-training for 4 epochs = 0.524\n",
    "\n",
    "after 4 accuracy comes down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5e5ac3ed-e19e-43e9-846f-f30e372d3a18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:02<00:00,  6.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy before finetuning : 0.524\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Test Accuracy before finetuning : {get_acc(classifier,test_loader,device)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f83e20-7ef2-4c44-8b0e-7cd87d5cebeb",
   "metadata": {},
   "source": [
    "PHASE 2 finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "86c3e434-5e53-472c-83c2-c6e12c5aa38a",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_optimizer = torch.optim.AdamW(classifier.parameters(), lr=2e-5)\n",
    "classification_loss_fn = nn.CrossEntropyLoss(weight = class_weights)\n",
    "generation_loss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5aa6b03a-9d3a-4e27-9be9-9ccb288757ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classifier(model, dataloader, testloader, optimizer,classification_loss_fn,generation_loss_fn, device, epochs):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "        for batch in progress_bar:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            label = batch['label'].to(device)\n",
    "            long_answer_ids = batch['long_answer_ids'].to(device)\n",
    "\n",
    "\n",
    "            classification_logits,long_answer_logits = model(input_ids, attention_mask)\n",
    "            \n",
    "            # Compute losses\n",
    "            classification_loss = classification_loss_fn(classification_logits, label)\n",
    "            # classification_loss = CB_loss(label.to('cpu'), classification_logits.to('cpu'), class_count_list, num_classes,loss_type, beta, gamma)\n",
    "            generation_loss = generation_loss_fn(long_answer_logits.view(-1, long_answer_logits.size(-1)), long_answer_ids.view(-1))\n",
    "    \n",
    "            # Combine losses\n",
    "            loss = classification_loss + generation_loss\n",
    "      \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "                \n",
    "            total_loss += loss.item()\n",
    "            progress_bar.set_postfix({'loss': loss.item()})\n",
    "            \n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(dataloader):.4f}\")\n",
    "        print(f\"Test Accuracy : {get_acc(model,testloader,device)}\")\n",
    "        model.train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "56d21f09-5bfe-49a5-a2f1-9553810c4fe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/1: 100%|██████████| 26472/26472 [45:56<00:00,  9.60it/s, loss=7.45] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1, Loss: 7.2657\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 16/16 [00:02<00:00,  6.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy : 0.614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_classifier(classifier,labeled_loader,test_loader,classifier_optimizer,classification_loss_fn,\n",
    "                 generation_loss_fn,device,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1172b1c8-c744-4ee9-a897-207c1ccd7e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "state = {\n",
    "    'state_dict': classifier.state_dict(),\n",
    "    'optimizer': classifier_optimizer.state_dict(),\n",
    "    'epochs': 2,\n",
    "    'lr':2e-5\n",
    "}\n",
    "\n",
    "torch.save(state, f\"weights/{model_name.split('/')[1]}_contrastive_QA_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8b468e-cdb7-4772-aade-0dfde9fcf350",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
